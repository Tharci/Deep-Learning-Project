{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess import *\n",
    "from utils import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Image loading and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3407 images belonging to 1 classes.\n",
      "Found 851 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "crop_size = (256, 256)\n",
    "\n",
    "def preprocess_generator_train_test(generator):\n",
    "    while True:\n",
    "        batch = next(generator)\n",
    "        batch_crops = np.zeros((batch.shape[0], crop_size[0], crop_size[1], 3))\n",
    "        for i in range(batch.shape[0]):\n",
    "            batch_crops[i] = crop_img(batch[i], crop_size)\n",
    "            batch_crops[i] = center_img(batch_crops[i]) # shift to -1,1\n",
    "        yield batch_crops, batch_crops\n",
    "\n",
    "\n",
    "def slice_img(img, slice_size):\n",
    "    padded_size = (math.ceil(img.shape[0] / float(slice_size[0])) * slice_size[0],\n",
    "                math.ceil(img.shape[1] / float(slice_size[1])) * slice_size[1],\n",
    "                img.shape[2])\n",
    "\n",
    "    padded_img = np.zeros(padded_size)\n",
    "    padded_img[0:img.shape[0], 0:img.shape[1]] = img\n",
    "    M, N = slice_size\n",
    "    slices = np.array(\n",
    "            [padded_img[x:x+M, y:y+N] for x in range(0, padded_img.shape[0], M)\n",
    "                                      for y in range(0, padded_img.shape[1], N)]\n",
    "    )\n",
    "    return slices\n",
    "\n",
    "\n",
    "def deslice_img(slices, img_size):\n",
    "    slice_size = slices[0].shape\n",
    "    img = np.zeros(img_size)\n",
    "\n",
    "    y, x = 0, 0\n",
    "    for s in slices:\n",
    "        y_to = min(y+slice_size[0], img.shape[0])\n",
    "        x_to = min(x+slice_size[1], img.shape[1])\n",
    "        img[y:y_to, x:x_to] = s[0:y_to-y, 0:x_to-x]\n",
    "        if x + slice_size[1] >= img.shape[1]:\n",
    "            x = 0\n",
    "            y += slice_size[0]\n",
    "        else:\n",
    "            x += slice_size[1]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_generator_slice(generator):\n",
    "    while True:\n",
    "        batch = next(generator)\n",
    "        for img in batch:\n",
    "            img = center_img(img)\n",
    "            yield slice_img(img, crop_size)\n",
    "\n",
    "\n",
    "downscale_fact = 2\n",
    "image_height_orig = 720\n",
    "image_width_orig = 1280\n",
    "image_height_ds = image_height_orig // downscale_fact\n",
    "image_width_ds = image_width_orig // downscale_fact\n",
    "\n",
    "train_ds, test_ds = create_dataflows('../images', (image_height_ds, image_width_ds), 16)\n",
    "\n",
    "train_ds_prep = preprocess_generator_train_test(train_ds)\n",
    "test_ds_prep = preprocess_generator_train_test(test_ds)\n",
    "\n",
    "# train_ds_mapped = pair_mapping(train_ds)\n",
    "# test_ds_mapped = pair_mapping(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "images = next(train_ds_prep)\n",
    "for i, image in enumerate(images[0][:2]):\n",
    "    ax = plt.subplot(5, 1, i + 1)\n",
    "    plt.imshow(decenter_img(image))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "train_ds.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Model of the AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ssim_loss(y_true, y_pred):\n",
    "    return 1 - tensorflow.reduce_mean(tensorflow.image.ssim_multiscale(decenter_img(y_true), decenter_img(y_pred), 1.0, filter_size=3))\n",
    "\n",
    "total_variation_weight = 1e-10\n",
    "\n",
    "def tv_loss(y_true, y_pred):\n",
    "    # tv = tf.reduce_sum(tf.image.total_variation(decenter_img(y_pred))) # tv1: total var of y_pred\n",
    "    # tv = tf.reduce_sum(tf.image.total_variation(tf.abs(decenter_img(y_pred) - decenter_img(y_true)))) # tv3, no high hopes, just bull\n",
    "    tv = tf.nn.relu(tf.reduce_sum(tf.image.total_variation(decenter_img(y_pred))) - tf.reduce_sum(tf.image.total_variation(decenter_img(y_true)))) # tv4\n",
    "    return ssim_loss(y_true, y_pred) + total_variation_weight * tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K, losses\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Activation, ZeroPadding2D, BatchNormalization, Conv2DTranspose, UpSampling2D, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "def create_improved_baseline_model_12x_comp(image_size):\n",
    "    ### Encoder ###\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, kernel_size=(7, 7), strides=(2, 2), kernel_initializer='he_normal',\n",
    "                     padding='same', input_shape=(image_size[0], image_size[1], 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('leaky_relu'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3), kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('leaky_relu'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3), kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('leaky_relu'))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('leaky_relu'))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('leaky_relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), strides=(2, 2), kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('leaky_relu'))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), strides=(2, 2), kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('leaky_relu', name='encoded', dtype='float16'))\n",
    "\n",
    "\n",
    "    ### Decoder ###\n",
    "\n",
    "    model.add(Conv2DTranspose(64, kernel_size=(3, 3), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    # model.add(BatchNormalization()) #\n",
    "    model.add(Activation('leaky_relu'))\n",
    "\n",
    "    # model.add(Conv2DTranspose(32, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')) #\n",
    "    # model.add(BatchNormalization()) #\n",
    "    # model.add(Activation('leaky_relu')) #\n",
    "\n",
    "    model.add(Conv2DTranspose(32, kernel_size=(3, 3), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    # model.add(BatchNormalization()) #\n",
    "    model.add(Activation('leaky_relu'))\n",
    "\n",
    "    # model.add(Conv2DTranspose(16, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')) #\n",
    "    # model.add(BatchNormalization()) #\n",
    "    # model.add(Activation('leaky_relu')) #\n",
    "\n",
    "    # model.add(Conv2DTranspose(16, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')) #\n",
    "    # model.add(BatchNormalization()) #\n",
    "    # model.add(Activation('leaky_relu')) #\n",
    "\n",
    "    model.add(Conv2DTranspose(16, kernel_size=(3, 3), strides=(2, 2), padding='same', kernel_initializer='he_normal'))\n",
    "    # model.add(BatchNormalization()) #\n",
    "    model.add(Activation('leaky_relu'))\n",
    "\n",
    "    model.add(Conv2DTranspose(3, kernel_size=(7, 7), strides=(2, 2), activation='tanh', padding='same', name='decoded'))\n",
    "\n",
    "    print((\"shape of encoded\", K.int_shape(model.get_layer('encoded').output)))\n",
    "    print((\"shape of decoded\", K.int_shape(model.get_layer('decoded').output)))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('shape of encoded', (None, 16, 16, 32))\n",
      "('shape of decoded', (None, 256, 256, 3))\n"
     ]
    }
   ],
   "source": [
    "autoencoder = create_improved_baseline_model_12x_comp(crop_size)\n",
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0025), loss=tv_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training the AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note: Delete the logs before running this. The command below should do this, but I would prefer to do it manually.\n",
    "# !RMDIR \"./logs/\" /S /Q\n",
    "\n",
    "# Launching Tensorboard\n",
    "%tensorboard --logdir ./logs/fit --host localhost #--port:6006\n",
    "# localhost:6006 in browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger, TensorBoard\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Set APPEND=TRUE if you are continuing the training, so that the log.csv wouldn't be reset!\n",
    "csv_logger = CSVLogger('log.csv', append=False, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "212/212 [==============================] - 43s 182ms/step - loss: 0.1564 - val_loss: 0.1660\n",
      "Epoch 2/55\n",
      "212/212 [==============================] - 38s 178ms/step - loss: 0.0847 - val_loss: 0.0816\n",
      "Epoch 3/55\n",
      "212/212 [==============================] - 39s 186ms/step - loss: 0.0738 - val_loss: 0.0835\n",
      "Epoch 4/55\n",
      "212/212 [==============================] - 38s 179ms/step - loss: 0.0688 - val_loss: 0.0666\n",
      "Epoch 5/55\n",
      "212/212 [==============================] - 36s 170ms/step - loss: 0.0651 - val_loss: 0.1231\n",
      "Epoch 6/55\n",
      "212/212 [==============================] - 36s 171ms/step - loss: 0.0643 - val_loss: 0.2710\n",
      "Epoch 7/55\n",
      "212/212 [==============================] - 36s 170ms/step - loss: 0.0645 - val_loss: 0.0565\n",
      "Epoch 8/55\n",
      "212/212 [==============================] - 36s 170ms/step - loss: 0.0621 - val_loss: 0.0822\n",
      "Epoch 9/55\n",
      "212/212 [==============================] - 37s 174ms/step - loss: 0.0596 - val_loss: 0.0653\n",
      "Epoch 10/55\n",
      "212/212 [==============================] - 37s 173ms/step - loss: 0.0583 - val_loss: 0.0548\n",
      "Epoch 11/55\n",
      "212/212 [==============================] - 38s 178ms/step - loss: 0.0561 - val_loss: 0.0641\n",
      "Epoch 12/55\n",
      "212/212 [==============================] - 36s 171ms/step - loss: 0.0550 - val_loss: 0.0531\n",
      "Epoch 13/55\n",
      "212/212 [==============================] - 36s 171ms/step - loss: 0.0545 - val_loss: 0.0584\n",
      "Epoch 14/55\n",
      "212/212 [==============================] - 36s 169ms/step - loss: 0.0525 - val_loss: 0.0457\n",
      "Epoch 15/55\n",
      "212/212 [==============================] - 36s 170ms/step - loss: 0.0516 - val_loss: 0.0667\n",
      "Epoch 16/55\n",
      "212/212 [==============================] - 38s 178ms/step - loss: 0.0487 - val_loss: 0.0921\n",
      "Epoch 17/55\n",
      "212/212 [==============================] - 38s 180ms/step - loss: 0.0464 - val_loss: 0.0572\n",
      "Epoch 18/55\n",
      "212/212 [==============================] - 37s 176ms/step - loss: 0.0452 - val_loss: 0.0621\n",
      "Epoch 19/55\n",
      "212/212 [==============================] - 36s 170ms/step - loss: 0.0425 - val_loss: 0.0525\n",
      "Epoch 20/55\n",
      "212/212 [==============================] - 36s 170ms/step - loss: 0.0417 - val_loss: 0.0541\n",
      "Epoch 21/55\n",
      "212/212 [==============================] - 38s 179ms/step - loss: 0.0418 - val_loss: 0.0606\n",
      "Epoch 22/55\n",
      "212/212 [==============================] - 37s 177ms/step - loss: 0.0393 - val_loss: 0.0439\n",
      "Epoch 23/55\n",
      "212/212 [==============================] - 36s 172ms/step - loss: 0.0390 - val_loss: 0.0364\n",
      "Epoch 24/55\n",
      "212/212 [==============================] - 36s 168ms/step - loss: 0.0389 - val_loss: 0.0501\n",
      "Epoch 25/55\n",
      "212/212 [==============================] - 37s 176ms/step - loss: 0.0380 - val_loss: 0.0459\n",
      "Epoch 26/55\n",
      "212/212 [==============================] - 36s 172ms/step - loss: 0.0375 - val_loss: 0.0488\n",
      "Epoch 27/55\n",
      "212/212 [==============================] - 36s 172ms/step - loss: 0.0375 - val_loss: 0.0369\n",
      "Epoch 28/55\n",
      "212/212 [==============================] - 35s 168ms/step - loss: 0.0365 - val_loss: 0.0425\n",
      "Epoch 29/55\n",
      "212/212 [==============================] - 36s 168ms/step - loss: 0.0367 - val_loss: 0.0354\n",
      "Epoch 30/55\n",
      "212/212 [==============================] - 38s 180ms/step - loss: 0.0367 - val_loss: 0.0394\n",
      "Epoch 31/55\n",
      "212/212 [==============================] - 36s 172ms/step - loss: 0.0351 - val_loss: 0.0429\n",
      "Epoch 32/55\n",
      "212/212 [==============================] - 36s 169ms/step - loss: 0.0357 - val_loss: 0.0441\n",
      "Epoch 33/55\n",
      "212/212 [==============================] - 36s 168ms/step - loss: 0.0346 - val_loss: 0.0375\n",
      "Epoch 34/55\n",
      "212/212 [==============================] - 36s 168ms/step - loss: 0.0344 - val_loss: 0.0420\n",
      "Epoch 35/55\n",
      "212/212 [==============================] - 35s 167ms/step - loss: 0.0340 - val_loss: 0.0388\n",
      "Epoch 36/55\n",
      "212/212 [==============================] - 35s 168ms/step - loss: 0.0340 - val_loss: 0.0330\n",
      "Epoch 37/55\n",
      "212/212 [==============================] - 36s 168ms/step - loss: 0.0338 - val_loss: 0.0410\n",
      "Epoch 38/55\n",
      "212/212 [==============================] - 36s 170ms/step - loss: 0.0348 - val_loss: 0.0732\n",
      "Epoch 39/55\n",
      "212/212 [==============================] - 39s 185ms/step - loss: 0.0338 - val_loss: 0.0457\n",
      "Epoch 40/55\n",
      "212/212 [==============================] - 41s 195ms/step - loss: 0.0343 - val_loss: 0.0633\n",
      "Epoch 41/55\n",
      "212/212 [==============================] - 39s 184ms/step - loss: 0.0328 - val_loss: 0.0465\n",
      "Epoch 42/55\n",
      "212/212 [==============================] - 36s 171ms/step - loss: 0.0327 - val_loss: 0.0335\n",
      "Epoch 43/55\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0331 - val_loss: 0.0360\n",
      "Epoch 44/55\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0332 - val_loss: 0.0375\n",
      "Epoch 45/55\n",
      "212/212 [==============================] - 35s 165ms/step - loss: 0.0322 - val_loss: 0.0325\n",
      "Epoch 46/55\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0326 - val_loss: 0.0346\n",
      "Epoch 47/55\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0327 - val_loss: 0.0354\n",
      "Epoch 48/55\n",
      "212/212 [==============================] - 35s 165ms/step - loss: 0.0325 - val_loss: 0.0326\n",
      "Epoch 49/55\n",
      "212/212 [==============================] - 35s 164ms/step - loss: 0.0315 - val_loss: 0.0405\n",
      "Epoch 50/55\n",
      "212/212 [==============================] - 36s 168ms/step - loss: 0.0314 - val_loss: 0.0302\n",
      "Epoch 51/55\n",
      "212/212 [==============================] - 36s 171ms/step - loss: 0.0317 - val_loss: 0.0365\n",
      "Epoch 52/55\n",
      "212/212 [==============================] - 37s 173ms/step - loss: 0.0311 - val_loss: 0.0482\n",
      "Epoch 53/55\n",
      "212/212 [==============================] - 36s 172ms/step - loss: 0.0313 - val_loss: 0.0327\n",
      "Epoch 54/55\n",
      "212/212 [==============================] - 35s 166ms/step - loss: 0.0317 - val_loss: 0.0449\n",
      "Epoch 55/55\n",
      "212/212 [==============================] - 36s 169ms/step - loss: 0.0312 - val_loss: 0.0301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27a90d55ac0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(train_ds_prep,\n",
    "                validation_data = test_ds_prep,\n",
    "                steps_per_epoch = train_ds.n // train_ds.batch_size,\n",
    "                validation_steps = test_ds.n // test_ds.batch_size,\n",
    "                epochs=55,\n",
    "                callbacks=[csv_logger, tensorboard_callback],\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 11). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model-saves/TV-TEST/tv_loss_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model-saves/TV-TEST/tv_loss_4/assets\n"
     ]
    }
   ],
   "source": [
    "# Saving the model:\n",
    "autoencoder.save('../model-saves/TV-TEST/tv_loss_4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the model:\n",
    "# autoencoder = keras.models.load_model('../model-saves/improved-12x-RGB-v2/', custom_objects={\n",
    "#     'ssim_loss': ssim_loss\n",
    "# })\n",
    "\n",
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Running the AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set = []\n",
    "for i in range(4):\n",
    "    test_set.append(next(preprocess_generator_slice(test_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img_size = (image_height_ds, image_width_ds, 3)\n",
    "\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "\n",
    "img_number = 4\n",
    "for i in range(img_number):\n",
    "    image_slices = test_set[i]\n",
    "    # image_slices = next(preprocess_generator_slice(test_ds))\n",
    "\n",
    "    decoded_slices = autoencoder.predict(image_slices)\n",
    "    decoded_img = deslice_img(decoded_slices, img_size)\n",
    "    decoded_img = decenter_img(decoded_img)\n",
    "\n",
    "    original_image = decenter_img(deslice_img(image_slices, img_size))\n",
    "\n",
    "    plt.subplot(img_number, 2, i*2+1)\n",
    "    plt.imshow(original_image)\n",
    "    plt.subplot(img_number, 2, i*2+2)\n",
    "    plt.imshow(decoded_img)\n",
    "\n",
    "# rescaled_img = cv2.resize(decenter_img(in_img), (in_img.shape[0] // 3, in_img.shape[1] // 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
